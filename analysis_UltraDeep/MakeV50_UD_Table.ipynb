{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7204413",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "In this file, we are showcasing the `v50 (km/s)` computation for every spectrum only for the frequently appeared absorption lines given in the dictionary `frequent_lines`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e5088",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Loading the libraries, files, and spectrums so we can run preliminary anaylsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2f4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from specutils import Spectrum1D, SpectralRegion\n",
    "from astropy.io import fits \n",
    "\n",
    "from specutils.manipulation import noise_region_uncertainty, extract_region\n",
    "\n",
    "# ===========================\n",
    "#          CONSTANTS\n",
    "# ===========================\n",
    "u_wavelength = u.Angstrom \n",
    "u_flux       = u.erg / (u.s ** 2) / (u.cm ** 2) / u.Angstrom\n",
    "u_velocity   = u.km / u.second\n",
    "\n",
    "light_speed  = 299792.458 * u_velocity\n",
    "\n",
    "frequent_lines = {\n",
    "    'C II λ1334': 1334, # 8 times\n",
    "    'O III λ1666': 1665.85, # 20 times\n",
    "    'C II λ2326': 2326.0, # 16 times\n",
    "    'Fe II λ2344': 2344, # 30 times\n",
    "    'Fe II λ2374': 2374, # 35 times\n",
    "    'Ne IV λ2440': 2439.5, # 18 times\n",
    "    'Fe II λ2586': 2586, # 25 times\n",
    "    'Fe II λ2600': 2600, # 26 times\n",
    "    'Mg II λ2799': 2799.117, # 26 times\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a6f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Loaded Table from HDF5 =====\n",
      ">>> The table has been successfully loaded from 'UltraDeep_Population_Profile.h5' with the key 'PopulationProfile'.\n",
      "===== Loaded Absorption Line Detection Table from HDF5 =====\n",
      ">>> The table has been successfully loaded from 'UltraDeep_Population_Profile.h5' with the key 'AbsorptionLineDetection'.\n"
     ]
    }
   ],
   "source": [
    "hdf5_path = \"UltraDeep_Population_Profile.h5\"\n",
    "with pd.HDFStore(hdf5_path, mode='r') as store:\n",
    "    profile_table = store.get(\"PopulationProfile\")  # Load the DataFrame from the HDF5 file\n",
    "    print(f\"\"\"===== Loaded Table from HDF5 =====\n",
    ">>> The table has been successfully loaded from '{hdf5_path}' with the key 'PopulationProfile'.\"\"\") \n",
    "    \n",
    "    absorption_table = store.get(\"AbsorptionLineDetection\")\n",
    "    print(f\"\"\"===== Loaded Absorption Line Detection Table from HDF5 =====\n",
    ">>> The table has been successfully loaded from '{hdf5_path}' with the key 'AbsorptionLineDetection'.\"\"\")\n",
    "\n",
    "redshift = profile_table['Z'].values\n",
    "fits_num = profile_table['NUM'].values\n",
    "zflag = profile_table['ZFLAGS'].values\n",
    "stellar_mass = profile_table[\"STELLAR_MASS\"].values # This is in log scale\n",
    "sfr = profile_table[\"SFR\"].values\n",
    "\n",
    "redshift = redshift[zflag == 4]\n",
    "fits_num = fits_num[zflag == 4]\n",
    "stellar_mass = stellar_mass[zflag == 4]\n",
    "sfr = sfr[zflag == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df29fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Known lines dictionary\n",
    "known_line = {\n",
    "    'O VI λ1034': 1033.82,\n",
    "    'Lyα λ1215': 1215.24,\n",
    "    'N V λ1241': 1240.81,\n",
    "    'O I λ1306': 1305.53,\n",
    "    'C II λ1335': 1335.31,\n",
    "    'Si IV λ1398': 1397.61,\n",
    "    'Si IV + O IV λ1400': 1399.8,\n",
    "    'C IV λ1549': 1549.48,\n",
    "    'He II λ1640': 1640.4,\n",
    "    'O III λ1666': 1665.85,\n",
    "    'Al III λ1857': 1857.4,\n",
    "    'C III λ1909': 1908.734,\n",
    "    'C II λ2326': 2326.0,\n",
    "    'Ne IV λ2440': 2439.5,\n",
    "    'Mg II λ2799': 2799.117,\n",
    "    'Ne V λ3347': 3346.79,\n",
    "    'Ne VI λ3427': 3426.85,\n",
    "    'O II λ3727': 3727.092,\n",
    "    'O II λ3730': 3729.875,\n",
    "    'He I λ3889': 3889.0,\n",
    "    'K λ3935': 3934.777,\n",
    "    'H λ3970': 3969.588,\n",
    "    'Hδ λ4103': 4102.89,\n",
    "    'G λ4306': 4305.61,\n",
    "    'Hγ λ4342': 4341.68,\n",
    "    'O III λ4364': 4364.436,\n",
    "    'Hβ λ4863': 4862.68,\n",
    "    'O III λ4933': 4932.603,\n",
    "    'O III λ4960': 4960.295,\n",
    "    'O III λ5008': 5008.240,\n",
    "    'Mg λ5177': 5176.7,\n",
    "    'Na λ5896': 5895.6,\n",
    "    'O I λ6302': 6302.046,\n",
    "    'O I λ6366': 6365.536,\n",
    "    'N I λ6529': 6529.03,\n",
    "    'N II λ6550': 6549.86,\n",
    "    'Hα λ6565': 6564.61,\n",
    "    'N II λ6585': 6585.27,\n",
    "    'S II λ6718': 6718.29,\n",
    "    'S II λ6733': 6732.67,\n",
    "    'CaII λ8500': 8500.36,\n",
    "    'CaII λ8544': 8544.44,\n",
    "    'CaII λ8665': 8664.52,\n",
    "    'Fe II λ2586': 2586,\n",
    "    'Fe II λ2600': 2600,\n",
    "    'Fe II λ2344': 2344,\n",
    "    'Fe II λ2374': 2374\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53735a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e504b",
   "metadata": {},
   "source": [
    "## Continuum crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find first zero crossing on each side of systemic velocity\n",
    "def find_zero_crossings(velocity, flux):\n",
    "    \"\"\"\n",
    "    Find the first zero crossings (where flux crosses 1.0) on each side of systemic velocity (v=0).\n",
    "    Doesn't care if it's an absorption or emission feature, just the crossing points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocity : np.ndarray\n",
    "        Velocity array in km/s\n",
    "    flux : np.ndarray\n",
    "        Normalized flux array (continuum is at 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    left_crossing : float or None\n",
    "        Velocity of first zero crossing on the left side (negative velocity)\n",
    "    right_crossing : float or None\n",
    "        Velocity of first zero crossing on the right side (positive velocity)\n",
    "    \"\"\"\n",
    "    # Subtract 1 so that zero crossings are at flux=0\n",
    "    flux_shifted = flux - 1.0\n",
    "    \n",
    "    # Find indices where velocity < 0 (left side) and > 0 (right side)\n",
    "    left_mask = velocity < 0\n",
    "    right_mask = velocity > 0\n",
    "    \n",
    "    left_crossing = None\n",
    "    right_crossing = None\n",
    "    \n",
    "    # Find first zero crossing on the LEFT side (moving from negative velocity to zero velocity)\n",
    "    if np.any(left_mask):\n",
    "        left_vel = velocity[left_mask]\n",
    "        left_flux = flux_shifted[left_mask]\n",
    "        \n",
    "        # Sort by velocity (most negative to zero)\n",
    "        sort_idx = np.argsort(left_vel)\n",
    "        left_vel_sorted = left_vel[sort_idx]\n",
    "        left_flux_sorted = left_flux[sort_idx]\n",
    "        \n",
    "        # Find sign changes (zero crossings)\n",
    "        sign_changes = np.where(np.diff(np.sign(left_flux_sorted)))[0]\n",
    "        \n",
    "        if len(sign_changes) > 0:\n",
    "            # Get the LAST sign change on the left (closest to v=0)\n",
    "            idx = sign_changes[-1]\n",
    "            # Linear interpolation to find exact crossing point\n",
    "            v1, v2 = left_vel_sorted[idx], left_vel_sorted[idx + 1]\n",
    "            f1, f2 = left_flux_sorted[idx], left_flux_sorted[idx + 1]\n",
    "            if f2 != f1:\n",
    "                left_crossing = v1 + (0 - f1) * (v2 - v1) / (f2 - f1)\n",
    "    \n",
    "    # Find first zero crossing on the RIGHT side (moving from zero velocity to positive velocities)\n",
    "    if np.any(right_mask):\n",
    "        right_vel = velocity[right_mask]\n",
    "        right_flux = flux_shifted[right_mask]\n",
    "        \n",
    "        # Sort by velocity (zero to most positive)\n",
    "        sort_idx = np.argsort(right_vel)\n",
    "        right_vel_sorted = right_vel[sort_idx]\n",
    "        right_flux_sorted = right_flux[sort_idx]\n",
    "        \n",
    "        # Find sign changes (zero crossings)\n",
    "        sign_changes = np.where(np.diff(np.sign(right_flux_sorted)))[0]\n",
    "        \n",
    "        if len(sign_changes) > 0:\n",
    "            # Get the FIRST sign change on the right (closest to v=0)\n",
    "            idx = sign_changes[0]\n",
    "            # Linear interpolation to find exact crossing point\n",
    "            v1, v2 = right_vel_sorted[idx], right_vel_sorted[idx + 1]\n",
    "            f1, f2 = right_flux_sorted[idx], right_flux_sorted[idx + 1]\n",
    "            if f2 != f1:\n",
    "                right_crossing = v1 + (0 - f1) * (v2 - v1) / (f2 - f1)\n",
    "    \n",
    "    return left_crossing, right_crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4604b1",
   "metadata": {},
   "source": [
    "## Absorption area and V50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e004cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute absorption area within wings\n",
    "def compute_absorption_area(velocity, flux, left_crossing, right_crossing):\n",
    "    \"\"\"\n",
    "    Compute the area between continuum (flux=1) and normalized flux within the absorption wings\n",
    "    using trapezoidal integration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocity : np.ndarray\n",
    "        Velocity array in km/s\n",
    "    flux : np.ndarray\n",
    "        Normalized flux array (continuum is at 1.0)\n",
    "    left_crossing : float or None\n",
    "        Left wing boundary velocity\n",
    "    right_crossing : float or None\n",
    "        Right wing boundary velocity\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    area : float or None\n",
    "        Integrated area between continuum and flux (absorption strength)\n",
    "        Units: km/s (velocity integrated flux deficit)\n",
    "    \"\"\"\n",
    "    if left_crossing is None or right_crossing is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract data within the wings\n",
    "    mask = (velocity >= left_crossing) & (velocity <= right_crossing)\n",
    "    vel_region = velocity[mask]\n",
    "    flux_region = flux[mask]\n",
    "    \n",
    "    if len(vel_region) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Calculate area: integral of (1 - flux) dv\n",
    "    # This is the flux deficit below continuum\n",
    "    flux_deficit = 1.0 - flux_region\n",
    "    \n",
    "    # Use trapezoidal integration\n",
    "    area = np.trapz(flux_deficit, vel_region)\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f299b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute v50 (median velocity)\n",
    "def compute_v50(velocity, flux, left_crossing, right_crossing):\n",
    "    \"\"\"\n",
    "    Compute the v50 velocity - the velocity at which half of the absorption area \n",
    "    has been accumulated when integrating from the left wing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocity : np.ndarray\n",
    "        Velocity array in km/s\n",
    "    flux : np.ndarray\n",
    "        Normalized flux array (continuum is at 1.0)\n",
    "    left_crossing : float or None\n",
    "        Left wing boundary velocity\n",
    "    right_crossing : float or None\n",
    "        Right wing boundary velocity\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    v50 : float or None\n",
    "        Velocity at which half the absorption area is accumulated\n",
    "        Units: km/s\n",
    "    \"\"\"\n",
    "    if left_crossing is None or right_crossing is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract data within the wings\n",
    "    mask = (velocity >= left_crossing) & (velocity <= right_crossing)\n",
    "    vel_region = velocity[mask]\n",
    "    flux_region = flux[mask]\n",
    "    \n",
    "    if len(vel_region) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Calculate flux deficit\n",
    "    flux_deficit = 1.0 - flux_region\n",
    "    \n",
    "    # Compute cumulative area from left to right\n",
    "    cumulative_area = np.zeros(len(vel_region))\n",
    "    for i in range(1, len(vel_region)):\n",
    "        # Trapezoidal rule for each segment\n",
    "        dv = vel_region[i] - vel_region[i-1]\n",
    "        avg_deficit = (flux_deficit[i] + flux_deficit[i-1]) / 2.0\n",
    "        cumulative_area[i] = cumulative_area[i-1] + avg_deficit * dv\n",
    "    \n",
    "    # Total area\n",
    "    total_area = cumulative_area[-1]\n",
    "    \n",
    "    if total_area <= 0:\n",
    "        return None\n",
    "    \n",
    "    # Find where cumulative area reaches half\n",
    "    half_area = total_area / 2.0\n",
    "    \n",
    "    # Find the index where we cross half the area\n",
    "    idx = np.searchsorted(cumulative_area, half_area)\n",
    "    \n",
    "    if idx == 0:\n",
    "        return vel_region[0]\n",
    "    if idx >= len(vel_region):\n",
    "        return vel_region[-1]\n",
    "    \n",
    "    # Linear interpolation to get precise v50\n",
    "    v1, v2 = vel_region[idx-1], vel_region[idx]\n",
    "    a1, a2 = cumulative_area[idx-1], cumulative_area[idx]\n",
    "    \n",
    "    if a2 != a1:\n",
    "        v50 = v1 + (half_area - a1) * (v2 - v1) / (a2 - a1)\n",
    "    else:\n",
    "        v50 = v1\n",
    "    \n",
    "    return v50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1109d",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c207b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Get systemic velocity from redshift \n",
    "def vsys(z:float):\n",
    "    return light_speed * ((z + 1) ** 2 - 1) / ((z + 1) ** 2 + 1)\n",
    "\n",
    "# ----- Chech if a value is in spectral region\n",
    "def is_inregion(val, region:SpectralRegion):\n",
    "    \"\"\"Check if a value is inside a (singular) spectral region.\n",
    "\n",
    "    Args:\n",
    "        val (float * unit): A value to check\n",
    "        region (SpectralRegion): The spectral region of interest. The spectral region \n",
    "                                 must be singular in a sense that it can be written as (a, b).\n",
    "\n",
    "    Raises:\n",
    "        Exception: If unit of val and region isn't the same.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether or not the value of interest is inside the region.\n",
    "    \"\"\"\n",
    "    if val.unit != region.lower.unit:\n",
    "        raise Exception(f\"Unit Error: The units of inserted value is {val.unit} but SpectralRegion is in units {region.lower.unit}.\")\n",
    "    \n",
    "    if not isinstance(val, float):\n",
    "        val = val.value \n",
    "    return region.lower.value <= val and val <= region.upper.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91472c63",
   "metadata": {},
   "source": [
    "## Quiet regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a440eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Get quietregion in observed frame (around known lines)\n",
    "def get_quietregion(redshift:float, scale_threshold:float = 3) -> SpectralRegion:\n",
    "    \"\"\"Extract regions that might not have major emission line. The extracted region is a subset of \n",
    "    3500 A and 9467.18 A. This is done by deleting the region around known_lines.\n",
    "\n",
    "    Args:\n",
    "        redshift (float): the redshift of a galaxy\n",
    "        scale_threshold (float, optional): Sampling step of the wavelength in observed frame is 5.355 A. \n",
    "                                           So, we use this parameter to extend/shrink the deleted region. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        SpectralRegion: Spectral region of no major emission line regions.\n",
    "    \"\"\"\n",
    "    known_lam = np.array([*known_line.values()])\n",
    "    \n",
    "    # Sampling step = 5.355\n",
    "    known_lamobs = known_lam * (1 + redshift) # Known_lam in observed frame. \n",
    "    quiet_region = [(known_lamobs[i], known_lamobs[i+1]) for i in range(len(known_lam) - 1) if known_lamobs[i+1] - known_lamobs[i] >= 100]\n",
    "    \n",
    "    if quiet_region[0][1] > 9467.18: # Too high redshift, no quiet region from the available known line.\n",
    "        return None \n",
    "    \n",
    "    # lam_threshold is the distance between known line's center and the edge which we want to exclude from the quiet region\n",
    "    # we do this to avoid potential remnants of emission line or absorption line\n",
    "    lam_threshold = scale_threshold * 5.355\n",
    "    \n",
    "    return_region = SpectralRegion(0*u.nm, 1*u.nm) # Template and will be removed later\n",
    "    for index in range(len(quiet_region)):\n",
    "        a = quiet_region[index][0]\n",
    "        b = quiet_region[index][1]\n",
    "        \n",
    "        # We want only subregions bounded by 3500 AA and 9467.18 AA because that's the observable frame's boundary\n",
    "        if 3500 <= a and b <= 9467.18: \n",
    "            return_region += SpectralRegion((a + lam_threshold) * u.Angstrom, (b - lam_threshold) * u.Angstrom)\n",
    "        \n",
    "    del return_region[0]\n",
    "    return return_region\n",
    "\n",
    "# ----- Get the arithmetic average of the 1sigma noise within quiet region.\n",
    "def get_quietregion_sigma(lam:np.ndarray, noise:np.ndarray, redshift:float, scale_threshold:float = 3) -> float:\n",
    "    \"\"\"Calculate a representative standard deviation of the noise of the quiet region. This representative\n",
    "    sigma is done by averaging the (absolute value of) noise in quiet region.\n",
    "\n",
    "    Args:\n",
    "        lam (np.ndarray): _description_\n",
    "        noise (np.ndarray): _description_\n",
    "        redshift (float): _description_\n",
    "        scale_threshold (float, optional): _description_. Defaults to 3.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: _description_\n",
    "\n",
    "    Returns:\n",
    "        float: _description_\n",
    "    \"\"\"\n",
    "    if lam is None or noise is None:\n",
    "        raise ValueError(\"Need proper inputs!\")\n",
    "    \n",
    "    quietregion = get_quietregion(redshift, scale_threshold)\n",
    "    mask = get_quietregion_mask(lam, quietregion)\n",
    "    \n",
    "    if sum(mask) == 0:\n",
    "        return None\n",
    "    \n",
    "    return noise[mask].sum() / len(noise[mask])\n",
    "\n",
    "# ----- Get mask for points in quietregion\n",
    "def get_quietregion_mask(lam:np.ndarray, quietregion:SpectralRegion) -> np.ndarray[bool]:\n",
    "    return np.array([is_inregion(wavelength * u.Angstrom, quietregion) for wavelength in lam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fbd4c2",
   "metadata": {},
   "source": [
    "## Spectrum Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f0dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Binning data\n",
    "def get_bindata(data, bins, binsmode=\"med\"):\n",
    "    \"\"\"Obtain a binned array by various method of binning.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Numpy array of data of interest.\n",
    "        bins (int): Number of data points that we want 1 point in the binned data to represent. Like, if bins = 3\n",
    "                    then we use 1 point in the binned data to represent 3 points in the original data. \n",
    "        binsmode (str, optional): _description_. Defaults to \"med\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid mode of binning.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Numpy array of binned data.\n",
    "    \"\"\"\n",
    "    binsmode = binsmode.lower()\n",
    "    match binsmode:\n",
    "        case \"med\":\n",
    "            binned_data = np.array([np.median(data[i:i+bins]) for i in range(len(data) - (bins - 1))])\n",
    "        case \"mean\":\n",
    "            binned_data = np.array([np.mean(data[i:i+bins]) for i in range(len(data) - (bins - 1))])\n",
    "        case \"sum\":\n",
    "            binned_data = np.array([np.sum(data[i:i+bins]) for i in range(len(data) - (bins - 1))])\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid binsmode. Choose from 'med', 'mean', or 'sum'.\")\n",
    "    return binned_data\n",
    "\n",
    "# ----- Get atm_clean fits file of an observation field\n",
    "def get_cleanfitsfile(fieldname:str, file_index:int):\n",
    "    \"\"\"Obtain a .fits file of a spectrum from the downloaded .fits file of a certain observation field.\n",
    "\n",
    "    Args:\n",
    "        fieldname (str): Name of the observation field of interest. (\"ud\", \"df02\", \"dcdfs\")\n",
    "        file_index (int): Index of a specific field in the field's directory.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Spectrum1D of the spectrum and noise.\n",
    "    \"\"\"\n",
    "    match fieldname:\n",
    "        case \"ud\":\n",
    "            spec_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Ultra Deep\\\\atm_clean\\\\\" \n",
    "            noise_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Ultra Deep\\\\noise\\\\\" \n",
    "            spec_files = os.listdir(spec_dir)\n",
    "            noise_files = os.listdir(noise_dir)\n",
    "        case \"df02\":\n",
    "            spec_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Deep\\\\VVDS-F0226-04\\\\atm_clean\\\\\" \n",
    "            noise_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Deep\\\\VVDS-F0226-04\\\\noise\\\\\" \n",
    "            spec_files = os.listdir(spec_dir)\n",
    "            noise_files = os.listdir(noise_dir)\n",
    "        case \"dcdfs\":\n",
    "            spec_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Deep\\\\VVDS-CDFS\\\\atm_clean\\\\\" \n",
    "            noise_dir = \"..\\\\Flagged Data\\\\VIMOS VLT Deep\\\\VVDS-CDFS\\\\noise\\\\\" \n",
    "            spec_files = os.listdir(spec_dir)\n",
    "            noise_files = os.listdir(noise_dir)\n",
    "\n",
    "    spec_file = fits.open(spec_dir + spec_files[file_index], memmap = True)\n",
    "    noise_file = fits.open(noise_dir + noise_files[file_index], memmap = True)\n",
    "    \n",
    "    return spec_file, noise_file\n",
    "\n",
    "# ----- Get Spectrum1D object from fits file.\n",
    "def get_spec1d(fieldname, file_index, redshift, bins = 1, binsmode = \"med\", fits_num = None, quiet_fluxsigma = True):\n",
    "    \"\"\"Obtain Spectrum1D object of a spectrum from the .fits file of a certain observation field.\n",
    "\n",
    "    Args:\n",
    "        fieldname (str): Name of the observation field of interest.\n",
    "        file_index (int): Index of spectrum .fits file within the field's directory.\n",
    "        redshift (float): redshift of the galaxy of interest.\n",
    "        bins (int, opetional): number of data points taht we want 1 point in the binned data\n",
    "                               to represent. Defaults is 1. (no binning)\n",
    "        binsmode (str, optional): binning mode of the flux array. Defaults to median.\n",
    "        fits_num (int, optional): .fits ID in the VVDS database. Defaults to None.\n",
    "        quiet_fluxsigma (bool, optional): Whether or not to use the standard deviation of flux to represent\n",
    "                                          the uncertainty in quiet region. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Specturm1D: Spectrum1D object of the spectrum of interest.\n",
    "    \"\"\"\n",
    "    \n",
    "    if quiet_fluxsigma: \n",
    "        spec_fits, _ = get_cleanfitsfile(fieldname, file_index)\n",
    "    else:\n",
    "        spec_fits, noise_fits = get_cleanfitsfile(fieldname, file_index)\n",
    "        noise = noise_fits[0].data[0]\n",
    "    \n",
    "    frame = \"obs\" # Spectrum in fits file is in observed frame.\n",
    "    \n",
    "    spec_header = spec_fits[0].header\n",
    "    flux = spec_fits[0].data[0]\n",
    "    lam = spec_header['CRVAL1'] + spec_header['CDELT1'] * np.arange(len(flux))\n",
    "    \n",
    "    spec_meta = {\"fitsnum\": fits_num,\n",
    "                 \"frame\": frame, \n",
    "                 \"redshift\": redshift}\n",
    "\n",
    "    if bins > 1:\n",
    "        lam = get_bindata(lam, bins, binsmode = \"mean\")\n",
    "        flux = get_bindata(flux, bins, binsmode)\n",
    "        if not quiet_fluxsigma:\n",
    "            noise = get_bindata(noise, bins, binsmode)\n",
    "            spec_meta.update({\"noise\": noise * u_flux})\n",
    "    \n",
    "    if quiet_fluxsigma:\n",
    "        spec_obj = Spectrum1D(spectral_axis = lam * u_wavelength, flux = flux * u_flux)\n",
    "        quietsigma = noise_region_uncertainty(spec_obj, get_quietregion(redshift))\n",
    "        quietsigma = quietsigma.uncertainty.array[0]\n",
    "    else: \n",
    "        quietsigma = get_quietregion_sigma(lam, flux, redshift)\n",
    "        \n",
    "    spec_meta.update({\"quietsigma\": quietsigma * u_flux})\n",
    "    \n",
    "    spec = Spectrum1D(spectral_axis = lam * u_wavelength,\n",
    "                      flux = flux * u_flux,\n",
    "                      meta = spec_meta)\n",
    "    \n",
    "    return spec \n",
    "\n",
    "# ----- Get matching known lines\n",
    "def get_linematching(detected_lines, redshift, frame = \"obs\", line_type = \"absorption\", tolerance = 50):\n",
    "    \"\"\"Obtain a dictionary of matching known lines from detected lines from find_lines_threshold. The key of the output\n",
    "    dictionary is the candidate known line's name while the value is the detected line center wavelength (in Angstrom).\n",
    "\n",
    "    Args:\n",
    "        detected_lines (QTable): detected lines from find_lines_threshold\n",
    "        redshift (float): redshift of the spectrum of interest\n",
    "        frame (str, optional): frame of the output matching lines. Choose from \"obsserved\" or \"rest\". Defautls to \"obs\".\n",
    "        line_type (str, optional): Line type detected by find_lines_threshold that we want to match. Defaults to \"absorption\".\n",
    "        tolerance (int, optional): Maximum line deviation from known_line wavelength (in Angstrom). Defaults to 10.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid frame.\n",
    "\n",
    "    Returns:\n",
    "        dict: Matching known lines dictionary. Key: known line name, Value: deteceted line center wavelength (in Angstrom)\n",
    "    \"\"\"\n",
    "    # ----- Helper function -----\n",
    "    # Get the NEAREST match only from the matching list (there might be multiple detected_lam matching to 1 known_lam)\n",
    "    def get_nearest_match(matching, known_center): \n",
    "        candidates = [match for match in matching if match[1] == known_center]\n",
    "        if len(candidates) == 0:\n",
    "            return None \n",
    "        candidates.sort(key = lambda x: x[2]) # Sort by np.abs(detected_lam - known_center)\n",
    "        return candidates[0]\n",
    "    \n",
    "    if line_type in (\"absorption\", \"emission\"):\n",
    "        detected_lines = detected_lines[detected_lines['line_type'] == line_type]\n",
    "    \n",
    "    known_name = list(known_line.keys())\n",
    "    known_lam = np.array([*known_line.values()]) * (1 + redshift) # In observed frame \n",
    "    detected_lam = detected_lines['line_center'].to(u.Angstrom).value \n",
    "    matching = []\n",
    "    \n",
    "    for known_center in known_lam:\n",
    "        idx = np.abs(detected_lam - known_center).argmin()\n",
    "        if np.abs(detected_lam[idx] - known_center) < tolerance:\n",
    "            matching.append((detected_lam[idx], known_center, np.abs(detected_lam[idx] - known_center)))\n",
    "\n",
    "    final_match = [get_nearest_match(matching, known_center) for known_center in known_lam]\n",
    "    final_match = [match for match in final_match if match is not None]\n",
    "\n",
    "    match_dict = {}\n",
    "\n",
    "    for match in final_match:\n",
    "        detected_center, known_center, _ = match\n",
    "        known_idx = known_lam.tolist().index(known_center)\n",
    "        known_name_iter = known_name[known_idx]\n",
    "        match_dict[known_name_iter] = detected_center\n",
    "    \n",
    "    if frame[0] == \"o\":\n",
    "        return match_dict \n",
    "    elif frame[0] == \"r\":\n",
    "        # Convert to rest frame \n",
    "        match_dict = {name: lam / (1 + redshift) for name, lam in match_dict.items()}\n",
    "        return match_dict\n",
    "    else:\n",
    "        raise ValueError(\"Invalid frame. Choose from \\\"obs\\\" or \\\"rest\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7cad8",
   "metadata": {},
   "source": [
    "# Creating the Table containing v50's.\n",
    "\n",
    "In this section, we will create a `pandas.DataFrame` containing the `v50` of each spectral lines in the `frequent_lines` dictionary. If we can't find the `v50` of that line, we will let `v50 = None` (perhaps at that line it's an emission or the line isn't a part of the observed spectrum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b707ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "v50_table = pd.DataFrame(columns=[\"NUM\"] + list(frequent_lines.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefe8a0",
   "metadata": {},
   "source": [
    "## Getting v50's from every spectrums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ff4add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(x, a0, a1, a2, a3, a4):\n",
    "    return a0 + a1*x + a2*x**2 + a3*x**3 + a4*x**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1ba4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_index in range(len(redshift)):\n",
    "    # ----- Continuum normalization -----\n",
    "    z = redshift[file_index]\n",
    "    fits_id = fits_num[file_index]\n",
    "    spec = get_spec1d(\"ud\", file_index, z, fits_num = fits_id)\n",
    "    \n",
    "    popt, _ = curve_fit(poly_fit, spec.spectral_axis.value, spec.flux.value)\n",
    "    con_flux = poly_fit(spec.spectral_axis.value, *popt)\n",
    "    \n",
    "    connorm_flux = spec.flux.value / con_flux\n",
    "    spec_norm = Spectrum1D(spectral_axis = spec.spectral_axis, flux = connorm_flux * u_flux)\n",
    "    \n",
    "    # ----- v50 calculation for frequent absorption lines -----\n",
    "    absorption_results = {}\n",
    "\n",
    "    for idx, (line_name, line_center) in enumerate(frequent_lines.items()):\n",
    "        line_center_obs = line_center * (1 + z) * u.Angstrom\n",
    "        vel_axis = light_speed * (spec.spectral_axis - line_center_obs) / line_center_obs\n",
    "        \n",
    "        # Extract region around the line center\n",
    "        region = SpectralRegion(line_center_obs - 500 * u.Angstrom, line_center_obs + 500 * u.Angstrom)\n",
    "        spec_line = extract_region(spec_norm, region)\n",
    "        \n",
    "        vel_line = light_speed * (spec_line.spectral_axis - line_center_obs) / line_center_obs\n",
    "        flux_line = spec_line.flux.value\n",
    "        \n",
    "        left_crossing, right_crossing = find_zero_crossings(vel_line.value, flux_line)\n",
    "        area = compute_absorption_area(vel_line.value, flux_line, left_crossing, right_crossing)\n",
    "        v50 = compute_v50(vel_line.value, flux_line, left_crossing, right_crossing)\n",
    "        \n",
    "        absorption_results[line_name] = {\n",
    "            \"left_crossing\": left_crossing,\n",
    "            \"right_crossing\": right_crossing,\n",
    "            \"area\": area,\n",
    "            \"v50\": v50\n",
    "        }\n",
    "\n",
    "    v50_dict = {line_name: absorption_results[line_name][\"v50\"] for line_name in frequent_lines.keys()}\n",
    "    v50_tablerow = {\"NUM\": fits_id, **v50_dict}\n",
    "    v50_table.loc[len(v50_table)] = v50_tablerow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd220c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM</th>\n",
       "      <th>C II λ1334</th>\n",
       "      <th>O III λ1666</th>\n",
       "      <th>C II λ2326</th>\n",
       "      <th>Fe II λ2344</th>\n",
       "      <th>Fe II λ2374</th>\n",
       "      <th>Ne IV λ2440</th>\n",
       "      <th>Fe II λ2586</th>\n",
       "      <th>Fe II λ2600</th>\n",
       "      <th>Mg II λ2799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910161060</td>\n",
       "      <td>2481.932745</td>\n",
       "      <td>-43.843288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.501362</td>\n",
       "      <td>536.563822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.001427</td>\n",
       "      <td>-1092.092585</td>\n",
       "      <td>-511.833830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910166908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910167444</td>\n",
       "      <td>1197.190109</td>\n",
       "      <td>530.574111</td>\n",
       "      <td>-168.393976</td>\n",
       "      <td>-31.857916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910170531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910170571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>426.719069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260.665260</td>\n",
       "      <td>55.154028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>910376825</td>\n",
       "      <td>-605.609746</td>\n",
       "      <td>437.757986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-479.409238</td>\n",
       "      <td>39.593217</td>\n",
       "      <td>-504.763695</td>\n",
       "      <td>-926.180619</td>\n",
       "      <td>55.150986</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>910378108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600.886488</td>\n",
       "      <td>576.439525</td>\n",
       "      <td>-276.508260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-45.055786</td>\n",
       "      <td>-663.492378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>910378821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654.755599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>445.857186</td>\n",
       "      <td>425.827269</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>910379064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2746.280165</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>910379760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306.669568</td>\n",
       "      <td>-27.622502</td>\n",
       "      <td>-304.068407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NUM   C II λ1334  O III λ1666  C II λ2326  Fe II λ2344  \\\n",
       "0    910161060  2481.932745   -43.843288         NaN  -413.501362   \n",
       "1    910166908          NaN          NaN         NaN          NaN   \n",
       "2    910167444  1197.190109   530.574111 -168.393976   -31.857916   \n",
       "3    910170531          NaN          NaN         NaN          NaN   \n",
       "4    910170571          NaN          NaN         NaN          NaN   \n",
       "..         ...          ...          ...         ...          ...   \n",
       "144  910376825  -605.609746   437.757986         NaN  -479.409238   \n",
       "145  910378108          NaN          NaN         NaN   600.886488   \n",
       "146  910378821          NaN          NaN         NaN          NaN   \n",
       "147  910379064          NaN          NaN         NaN          NaN   \n",
       "148  910379760          NaN          NaN         NaN          NaN   \n",
       "\n",
       "     Fe II λ2374  Ne IV λ2440  Fe II λ2586  Fe II λ2600  Mg II λ2799  \n",
       "0     536.563822          NaN   525.001427 -1092.092585  -511.833830  \n",
       "1            NaN          NaN          NaN          NaN          NaN  \n",
       "2            NaN          NaN          NaN          NaN          NaN  \n",
       "3            NaN          NaN          NaN          NaN          NaN  \n",
       "4     426.719069          NaN   260.665260    55.154028          NaN  \n",
       "..           ...          ...          ...          ...          ...  \n",
       "144    39.593217  -504.763695  -926.180619    55.150986          NaN  \n",
       "145   576.439525  -276.508260          NaN   -45.055786  -663.492378  \n",
       "146   654.755599          NaN   445.857186   425.827269          NaN  \n",
       "147          NaN          NaN          NaN  2746.280165          NaN  \n",
       "148          NaN          NaN   306.669568   -27.622502  -304.068407  \n",
       "\n",
       "[149 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v50_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f551d",
   "metadata": {},
   "source": [
    "## Transforming to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39b04cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Saved v50 Table to HDF5 =====\n",
      ">>> The v50 table has been successfully saved to 'UltraDeep_Population_Profile.h5' with the key 'Absorption_V50_Table'.\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(hdf5_path, mode='a') as store:\n",
    "    store.put(\"Absorption_V50_Table\", v50_table, format='table')\n",
    "    print(f\"\"\"===== Saved v50 Table to HDF5 =====\n",
    ">>> The v50 table has been successfully saved to '{hdf5_path}' with the key 'Absorption_V50_Table'.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
